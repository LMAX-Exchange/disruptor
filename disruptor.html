<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.12">
<meta name="author" content="Martin Thompson, Dave Farley, Michael Barker, Patricia Gee, Andrew Stewart">
<title>LMAX Disruptor: High performance alternative to bounded queues for exchanging data between concurrent threads</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
pre.rouge table td { padding: 5px; }
pre.rouge table pre { margin: 0; }
pre.rouge .cm {
  color: #999988;
  font-style: italic;
}
pre.rouge .cp {
  color: #999999;
  font-weight: bold;
}
pre.rouge .c1 {
  color: #999988;
  font-style: italic;
}
pre.rouge .cs {
  color: #999999;
  font-weight: bold;
  font-style: italic;
}
pre.rouge .c, pre.rouge .ch, pre.rouge .cd, pre.rouge .cpf {
  color: #999988;
  font-style: italic;
}
pre.rouge .err {
  color: #a61717;
  background-color: #e3d2d2;
}
pre.rouge .gd {
  color: #000000;
  background-color: #ffdddd;
}
pre.rouge .ge {
  color: #000000;
  font-style: italic;
}
pre.rouge .gr {
  color: #aa0000;
}
pre.rouge .gh {
  color: #999999;
}
pre.rouge .gi {
  color: #000000;
  background-color: #ddffdd;
}
pre.rouge .go {
  color: #888888;
}
pre.rouge .gp {
  color: #555555;
}
pre.rouge .gs {
  font-weight: bold;
}
pre.rouge .gu {
  color: #aaaaaa;
}
pre.rouge .gt {
  color: #aa0000;
}
pre.rouge .kc {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kd {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kn {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kp {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kr {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kt {
  color: #445588;
  font-weight: bold;
}
pre.rouge .k, pre.rouge .kv {
  color: #000000;
  font-weight: bold;
}
pre.rouge .mf {
  color: #009999;
}
pre.rouge .mh {
  color: #009999;
}
pre.rouge .il {
  color: #009999;
}
pre.rouge .mi {
  color: #009999;
}
pre.rouge .mo {
  color: #009999;
}
pre.rouge .m, pre.rouge .mb, pre.rouge .mx {
  color: #009999;
}
pre.rouge .sa {
  color: #000000;
  font-weight: bold;
}
pre.rouge .sb {
  color: #d14;
}
pre.rouge .sc {
  color: #d14;
}
pre.rouge .sd {
  color: #d14;
}
pre.rouge .s2 {
  color: #d14;
}
pre.rouge .se {
  color: #d14;
}
pre.rouge .sh {
  color: #d14;
}
pre.rouge .si {
  color: #d14;
}
pre.rouge .sx {
  color: #d14;
}
pre.rouge .sr {
  color: #009926;
}
pre.rouge .s1 {
  color: #d14;
}
pre.rouge .ss {
  color: #990073;
}
pre.rouge .s, pre.rouge .dl {
  color: #d14;
}
pre.rouge .na {
  color: #008080;
}
pre.rouge .bp {
  color: #999999;
}
pre.rouge .nb {
  color: #0086B3;
}
pre.rouge .nc {
  color: #445588;
  font-weight: bold;
}
pre.rouge .no {
  color: #008080;
}
pre.rouge .nd {
  color: #3c5d5d;
  font-weight: bold;
}
pre.rouge .ni {
  color: #800080;
}
pre.rouge .ne {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nf, pre.rouge .fm {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nl {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nn {
  color: #555555;
}
pre.rouge .nt {
  color: #000080;
}
pre.rouge .vc {
  color: #008080;
}
pre.rouge .vg {
  color: #008080;
}
pre.rouge .vi {
  color: #008080;
}
pre.rouge .nv, pre.rouge .vm {
  color: #008080;
}
pre.rouge .ow {
  color: #000000;
  font-weight: bold;
}
pre.rouge .o {
  color: #000000;
  font-weight: bold;
}
pre.rouge .w {
  color: #bbbbbb;
}
pre.rouge {
  background-color: #f8f8f8;
}
</style>
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>LMAX Disruptor: High performance alternative to bounded queues for exchanging data between concurrent threads</h1>
<div class="details">
<span id="author" class="author">Martin Thompson</span><br>
<span id="author2" class="author">Dave Farley</span><br>
<span id="author3" class="author">Michael Barker</span><br>
<span id="author4" class="author">Patricia Gee</span><br>
<span id="author5" class="author">Andrew Stewart</span><br>
<span id="revnumber">version 4.0.0-SNAPSHOT,</span>
<span id="revdate">May 2011</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_overview">1. Overview</a></li>
<li><a href="#_the_complexities_of_concurrency">2. The Complexities of Concurrency</a>
<ul class="sectlevel2">
<li><a href="#_the_cost_of_locks">2.1. The Cost of Locks</a></li>
<li><a href="#_the_costs_of_cas">2.2. The Costs of &#8220;CAS&#8221;</a></li>
<li><a href="#_memory_barriers">2.3. Memory Barriers</a></li>
<li><a href="#_cache_lines">2.4. Cache Lines</a></li>
<li><a href="#_the_problems_of_queues">2.5. The Problems of Queues</a></li>
<li><a href="#_pipelines_and_graphs">2.6. Pipelines and Graphs</a></li>
</ul>
</li>
<li><a href="#_design_of_the_lmax_disruptor">3. Design of the LMAX Disruptor</a>
<ul class="sectlevel2">
<li><a href="#_memory_allocation">3.1. Memory Allocation</a></li>
<li><a href="#_teasing_apart_the_concerns">3.2. Teasing Apart the Concerns</a></li>
<li><a href="#_sequencing">3.3. Sequencing</a></li>
<li><a href="#_batching_effect">3.4. Batching Effect</a></li>
<li><a href="#_dependency_graphs">3.5. Dependency Graphs</a></li>
<li><a href="#_disruptor_class_diagram">3.6. Disruptor Class Diagram</a></li>
<li><a href="#_code_example">3.7. Code Example</a></li>
</ul>
</li>
<li><a href="#_throughput_performance_testing">4. Throughput Performance Testing</a></li>
<li><a href="#_latency_performance_testing">5. Latency Performance Testing</a></li>
<li><a href="#_conclusion">6. Conclusion</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://github.com/LMAX-Exchange/disruptor" class="bare">https://github.com/LMAX-Exchange/disruptor</a></p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">Abstract</div>
<div class="paragraph">
<p>LMAX was established to create a very high performance financial exchange.
As part of our work to accomplish this goal we have evaluated several approaches to the design of such a system, but as we began to measure these we ran into some fundamental limits with conventional approaches.</p>
</div>
<div class="paragraph">
<p>Many applications depend on queues to exchange data between processing stages.
Our performance testing showed that the latency costs, when using queues in this way, were in the same order of magnitude as the cost of IO operations to disk (RAID or SSD based disk system) – dramatically slow.
If there are multiple queues in an end-to-end operation, this will add hundreds of microseconds to the overall latency.
There is clearly room for optimisation.</p>
</div>
<div class="paragraph">
<p>Further investigation and a focus on the computer science made us realise that the conflation of concerns inherent in conventional approaches, (e.g. queues and processing nodes) leads to contention in multi-threaded implementations, suggesting that there may be a better approach.</p>
</div>
<div class="paragraph">
<p>Thinking about how modern CPUs work, something we like to call &#8220;mechanical sympathy&#8221;, using good design practices with a strong focus on teasing apart the concerns, we came up with a data structure and a pattern of use that we have called the Disruptor.</p>
</div>
<div class="paragraph">
<p>Testing has shown that the mean latency using the Disruptor for a three-stage pipeline is 3 orders of magnitude lower than an equivalent queue-based approach.
In addition, the Disruptor handles approximately 8 times more throughput for the same configuration.</p>
</div>
<div class="paragraph">
<p>These performance improvements represent a step change in the thinking around concurrent programming.
This new pattern is an ideal foundation for any asynchronous event processing architecture where high-throughput and low-latency is required.</p>
</div>
<div class="paragraph">
<p>At LMAX we have built an order matching engine, real-time risk management, and a highly available in-memory transaction processing system all on this pattern to great success.
Each of these systems has set new performance standards that, as far as we can tell, are unsurpassed.</p>
</div>
<div class="paragraph">
<p>However this is not a specialist solution that is only of relevance in the Finance industry.
The Disruptor is a general-purpose mechanism that solves a complex problem in concurrent programming in a way that maximizes performance, and that is simple to implement.
Although some of the concepts may seem unusual it has been our experience that systems built to this pattern are significantly simpler to implement than comparable mechanisms.</p>
</div>
<div class="paragraph">
<p>The Disruptor has significantly less write contention, a lower concurrency overhead and is more cache friendly than comparable approaches, all of which results in greater throughput with less jitter at lower latency.
On processors at moderate clock rates we have seen over 25 million messages per second and latencies lower than 50 nanoseconds.
This performance is a significant improvement compared to any other implementation that we have seen.
This is very close to the theoretical limit of a modern processor to exchange data between cores.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">1. Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Disruptor is the result of our efforts to build the world’s highest performance financial exchange at LMAX.
Early designs focused on architectures derived from SEDA <sup class="footnote" id="_footnote_SEDA">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup> and Actors <sup class="footnote" id="_footnote_actors">[<a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote.">2</a>]</sup> using pipelines for throughput.
After profiling various implementations it became evident that the queuing of events between stages in the pipeline was dominating the costs.
We found that queues also introduced latency and high levels of jitter.
We expended significant effort on developing new queue implementations with better performance.
However it became evident that queues as a fundamental data structure are limited due to the conflation of design concerns for the producers, consumers, and their data storage.
The Disruptor is the result of our work to build a concurrent structure that cleanly separates these concerns.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_complexities_of_concurrency">2. The Complexities of Concurrency</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the context of this document, and computer science in general, concurrency means not only that two or more tasks happen in parallel, but also that they contend on access to resources.
The contended resource may be a database, file, socket or even a location in memory.</p>
</div>
<div class="paragraph">
<p>Concurrent execution of code is about two things, mutual exclusion and visibility of change.
Mutual exclusion is about managing contended updates to some resource.
Visibility of change is about controlling when such changes are made visible to other threads.
It is possible to avoid the need for mutual exclusion if you can eliminate the need for contended updates.
If your algorithm can guarantee that any given resource is modified by only one thread, then mutual exclusion is unnecessary.
Read and write operations require that all changes are made visible to other threads.
However only contended write operations require the mutual exclusion of the changes.</p>
</div>
<div class="paragraph">
<p>The most costly operation in any concurrent environment is a contended write access.
To have multiple threads write to the same resource requires complex and expensive coordination.
Typically this is achieved by employing a locking strategy of some kind.</p>
</div>
<div class="sect2">
<h3 id="_the_cost_of_locks">2.1. The Cost of Locks</h3>
<div class="paragraph">
<p>Locks provide mutual exclusion and ensure that the visibility of change occurs in an ordered manner.
Locks are incredibly expensive because they require arbitration when contended.
This arbitration is achieved by a context switch to the operating system kernel which will suspend threads waiting on a lock until it is released.
During such a context switch, as well as releasing control to the operating system which may decide to do other house-keeping tasks while it has control, execution context can lose previously cached data and instructions.
This can have a serious performance impact on modern processors.
Fast user mode locks can be employed but these are only of any real benefit when not contended.</p>
</div>
<div class="paragraph">
<p>We will illustrate the cost of locks with a simple demonstration.
The focus of this experiment is to call a function which increments a 64-bit counter in a loop 500 million times.
This can be executed by a single thread on a 2.4Ghz Intel Westmere EP in just 300ms if written in Java.
The language is unimportant to this experiment and results will be similar across all languages with the same basic primitives.</p>
</div>
<div class="paragraph">
<p>Once a lock is introduced to provide mutual exclusion, even when the lock is as yet un-contended, the cost goes up significantly.
The cost increases again, by orders of magnitude, when two or more threads begin to contend.
The results of this simple experiment are shown in the table below:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Comparative costs of contention</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Method</th>
<th class="tableblock halign-left valign-top">Time (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single thread</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">300</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single thread with lock</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Two threads with lock</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">224,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single thread with CAS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,700</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Two threads with CAS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">30,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single thread with volatile write</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4,700</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_the_costs_of_cas">2.2. The Costs of &#8220;CAS&#8221;</h3>
<div class="paragraph">
<p>A more efficient alternative to the use of locks can be employed for updating memory when the target of the update is a single word.
These alternatives are based upon the atomic, or interlocked, instructions implemented in modern processors.
These are commonly known as CAS (Compare And Swap) operations, e.g. &#8220;lock cmpxchg&#8221; on x86.
A CAS operation is a special machine-code instruction that allows a word in memory to be conditionally set as an atomic operation.
For the &#8220;increment a counter experiment&#8221; each thread can spin in a loop reading the counter then try to atomically set it to its new incremented value.
The old and new values are provided as parameters to this instruction.
If, when the operation is executed, the value of the counter matches the supplied expected value, the counter is updated with the new value.
If, on the other hand, the value is not as expected, the CAS operation will fail.
It is then up to the thread attempting to perform the change to retry, re-reading the counter incrementing from that value and so on until the change succeeds.
This CAS approach is significantly more efficient than locks because it does not require a context switch to the kernel for arbitration.
However CAS operations are not free of cost.
The processor must lock its instruction pipeline to ensure atomicity and employ a memory barrier to make the changes visible to other threads.
CAS operations are available in Java by using the <code>java.util.concurrent.Atomic*</code> classes.</p>
</div>
<div class="paragraph">
<p>If the critical section of the program is more complex than a simple increment of a counter it may take a complex state machine using multiple CAS operations to orchestrate the contention.
Developing concurrent programs using locks is difficult; developing lock-free algorithms using CAS operations and memory barriers is many times more complex and it is very difficult to prove that they are correct.</p>
</div>
<div class="paragraph">
<p>The ideal algorithm would be one with only a single thread owning all writes to a single resource with other threads reading the results.
To read the results in a multi-processor environment requires memory barriers to make the changes visible to threads running on other processors.</p>
</div>
</div>
<div class="sect2">
<h3 id="_memory_barriers">2.3. Memory Barriers</h3>
<div class="paragraph">
<p>Modern processors perform out-of-order execution of instructions and out-of-order loads and stores of data between memory and execution units for performance reasons.
The processors need only guarantee that program logic produces the same results regardless of execution order.
This is not an issue for single-threaded programs.
However, when threads share state it is important that all memory changes appear in order, at the point required, for the data exchange to be successful.
Memory barriers are used by processors to indicate sections of code where the ordering of memory updates is important.
They are the means by which hardware ordering and visibility of change is achieved between threads.
Compilers can put in place complimentary software barriers to ensure the ordering of compiled code, such software memory barriers are in addition to the hardware barriers used by the processors themselves.</p>
</div>
<div class="paragraph">
<p>Modern CPUs are now much faster than the current generation of memory systems.
To bridge this divide CPUs use complex cache systems which are effectively fast hardware hash tables without chaining.
These caches are kept coherent with other processor cache systems via message passing protocols.
In addition, processors have &#8220;store buffers&#8221; to offload writes to these caches, and &#8220;invalidate queues&#8221; so that the cache coherency protocols can acknowledge invalidation messages quickly for efficiency when a write is about to happen.</p>
</div>
<div class="paragraph">
<p>What this means for data is that the latest version of any value could, at any stage after being written, be in a register, a store buffer, one of many layers of cache, or in main memory.
If threads are to share this value, it needs to be made visible in an ordered fashion and this is achieved through the coordinated exchange of cache coherency messages.
The timely generation of these messages can be controlled by memory barriers.</p>
</div>
<div class="paragraph">
<p>A read memory barrier orders load instructions on the CPU that executes it by marking a point in the invalidate queue for changes coming into its cache.
This gives it a consistent view of the world for write operations ordered before the read barrier.</p>
</div>
<div class="paragraph">
<p>A write barrier orders store instructions on the CPU that executes it by marking a point in the store buffer, thus flushing writes out via its cache.
This barrier gives an ordered view to the world of what store operations happen before the write barrier.</p>
</div>
<div class="paragraph">
<p>A full memory barrier orders both loads and stores but only on the CPU that executes it.</p>
</div>
<div class="paragraph">
<p>Some CPUs have more variants in addition to these three primitives but these three are sufficient to understand the complexities of what is involved.
In the Java memory model the read and write of a volatile field implements the read and write barriers respectively.
This was made explicit in the Java Memory Model <sup class="footnote" id="_footnote_jmm">[<a id="_footnoteref_3" class="footnote" href="#_footnotedef_3" title="View footnote.">3</a>]</sup> as defined with the release of Java 5.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cache_lines">2.4. Cache Lines</h3>
<div class="paragraph">
<p>The way in which caching is used in modern processors is of immense importance to successful high performance operation.
Such processors are enormously efficient at churning through data and instructions held in cache and yet, comparatively, are massively inefficient when a cache miss occurs.</p>
</div>
<div class="paragraph">
<p>Our hardware does not move memory around in bytes or words.
For efficiency, caches are organised into cache-lines that are typically 32-256 bytes in size, the most common cache-line being 64 bytes.
This is the level of granularity at which cache coherency protocols operate.
This means that if two variables are in the same cache line, and they are written to by different threads, then they present the same problems of write contention as if they were a single variable.
This is a concept know as &#8220;false sharing&#8221;.
For high performance then, it is important to ensure that independent, but concurrently written, variables do not share the same cache-line if contention is to be minimised.</p>
</div>
<div class="paragraph">
<p>When accessing memory in a predictable manner CPUs are able to hide the latency cost of accessing main memory by predicting which memory is likely to be accessed next and pre-fetching it into the cache in the background.
This only works if the processors can detect a pattern of access such as walking memory with a predictable &#8220;stride&#8221;.
When iterating over the contents of an array the stride is predictable and so memory will be pre-fetched in cache lines, maximizing the efficiency of the access.
Strides typically have to be less than 2048 bytes in either direction to be noticed by the processor.
However, data structures like linked lists and trees tend to have nodes that are more widely distributed in memory with no predictable stride of access.
The lack of a consistent pattern in memory constrains the ability of the system to pre-fetch cache-lines, resulting in main memory accesses which can be more than 2 orders of magnitude less efficient.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_problems_of_queues">2.5. The Problems of Queues</h3>
<div class="paragraph">
<p>Queues typically use either linked-lists or arrays for the underlying storage of elements.
If an in-memory queue is allowed to be unbounded then for many classes of problem it can grow unchecked until it reaches the point of catastrophic failure by exhausting memory.
This happens when producers outpace the consumers.
Unbounded queues can be useful in systems where the producers are guaranteed not to outpace the consumers and memory is a precious resource, but there is always a risk if this assumption doesn’t hold and queue grows without limit.
To avoid this catastrophic outcome, queues are commonly constrained in size (bounded).
Keeping a queue bounded requires that it is either array-backed or that the size is actively tracked.</p>
</div>
<div class="paragraph">
<p>Queue implementations tend to have write contention on the head, tail, and size variables.
When in use, queues are typically always close to full or close to empty due to the differences in pace between consumers and producers.
They very rarely operate in a balanced middle ground where the rate of production and consumption is evenly matched.
This propensity to be always full or always empty results in high levels of contention and/or expensive cache coherence.
The problem is that even when the head and tail mechanisms are separated using different concurrent objects such as locks or CAS variables, they generally occupy the same cache-line.</p>
</div>
<div class="paragraph">
<p>The concerns of managing producers claiming the head of a queue, consumers claiming the tail, and the storage of nodes in between make the designs of concurrent implementations very complex to manage beyond using a single large-grain lock on the queue.
Large grain locks on the whole queue for put and take operations are simple to implement but represent a significant bottleneck to throughput.
If the concurrent concerns are teased apart within the semantics of a queue then the implementations become very complex for anything other than a single producer – single consumer implementation.</p>
</div>
<div class="paragraph">
<p>In Java there is a further problem with the use of queues, as they are significant sources of garbage.
Firstly, objects have to be allocated and placed in the queue.
Secondly, if linked-list backed, objects have to be allocated representing the nodes of the list.
When no longer referenced, all these objects allocated to support the queue implementation need to be re-claimed.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pipelines_and_graphs">2.6. Pipelines and Graphs</h3>
<div class="paragraph">
<p>For many classes of problem it makes sense to wire together several processing stages into pipelines. Such pipelines often have parallel paths, being organised into graph-like topologies.
The links between each stage are often implemented by queues with each stage having its own thread.</p>
</div>
<div class="paragraph">
<p>This approach is not cheap - at each stage we have to incur the cost of en-queuing and de-queuing units of work.
The number of targets multiplies this cost when the path must fork, and incurs an inevitable cost of contention when it must re-join after such a fork.</p>
</div>
<div class="paragraph">
<p>It would be ideal if the graph of dependencies could be expressed without incurring the cost of putting the queues between stages.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_design_of_the_lmax_disruptor">3. Design of the LMAX Disruptor</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While trying to address the problems described above, a design emerged through a rigorous separation of the concerns that we saw as being conflated in queues.
This approach was combined with a focus on ensuring that any data should be owned by only one thread for write access, therefore eliminating write contention.
That design became known as the &#8220;Disruptor&#8221;.
It was so named because it had elements of similarity for dealing with graphs of dependencies to the concept of &#8220;Phasers&#8221; <sup class="footnote" id="_footnote_phasers">[<a id="_footnoteref_4" class="footnote" href="#_footnotedef_4" title="View footnote.">4</a>]</sup> in Java 7, introduced to support Fork-Join.</p>
</div>
<div class="paragraph">
<p>The LMAX disruptor is designed to address all of the issues outlined above in an attempt to maximize the efficiency of memory allocation, and operate in a cache-friendly manner so that it will perform optimally on modern hardware.</p>
</div>
<div class="paragraph">
<p>At the heart of the disruptor mechanism sits a pre-allocated bounded data structure in the form of a ring-buffer.
Data is added to the ring buffer through one or more producers and processed by one or more consumers.</p>
</div>
<div class="sect2">
<h3 id="_memory_allocation">3.1. Memory Allocation</h3>
<div class="paragraph">
<p>All memory for the ring buffer is pre-allocated on start up.
A ring-buffer can store either an array of pointers to entries or an array of structures representing the entries.
The limitations of the Java language mean that entries are associated with the ring-buffer as pointers to objects.
Each of these entries is typically not the data being passed itself, but a container for it.
This pre-allocation of entries eliminates issues in languages that support garbage collection, since the entries will be re-used and live for the duration of the Disruptor instance.
The memory for these entries is allocated at the same time and it is highly likely that it will be laid out contiguously in main memory and so support cache striding.
There is a proposal by John Rose to introduce &#8220;value types&#8221; <sup class="footnote" id="_footnote_valuetypes">[<a id="_footnoteref_5" class="footnote" href="#_footnotedef_5" title="View footnote.">5</a>]</sup> to the Java language which would allow arrays of tuples, like other languages such as C, and so ensure that memory would be allocated contiguously and avoid the pointer indirection.</p>
</div>
<div class="paragraph">
<p>Garbage collection can be problematic when developing low-latency systems in a managed runtime environment like Java.
The more memory that is allocated the greater the burden this puts on the garbage collector.
Garbage collectors work at their best when objects are either very short-lived or effectively immortal.
The pre-allocation of entries in the ring buffer means that it is immortal as far as garbage collector is concerned and so represents little burden.</p>
</div>
<div class="paragraph">
<p>Under heavy load queue-based systems can back up, which can lead to a reduction in the rate of processing, and results in the allocated objects surviving longer than they should, thus being promoted beyond the young generation with generational garbage collectors.
This has two implications: first, the objects have to be copied between generations which cause latency jitter; second, these objects have to be collected from the old generation which is typically a much more expensive operation and increases the likelihood of &#8220;stop the world&#8221; pauses that result when the fragmented memory space requires compaction.
In large memory heaps this can cause pauses of seconds per GB in duration.</p>
</div>
</div>
<div class="sect2">
<h3 id="_teasing_apart_the_concerns">3.2. Teasing Apart the Concerns</h3>
<div class="paragraph">
<p>We saw the following concerns as being conflated in all queue implementations, to the extent that this collection of distinct behaviours tend to define the interfaces that queues implement:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Storage of items being exchanged</p>
</li>
<li>
<p>Coordination of producers claiming the next sequence for exchange</p>
</li>
<li>
<p>Coordination of consumers being notified that a new item is available</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>When designing a financial exchange in a language that uses garbage collection, too much memory allocation can be problematic.
So, as we have described linked-list backed queues are a not a good approach.
Garbage collection is minimized if the entire storage for the exchange of data between processing stages can be pre-allocated.
Further, if this allocation can be performed in a uniform chunk, then traversal of that data will be done in a manner that is very friendly to the caching strategies employed by modern processors.
A data-structure that meets this requirement is an array with all the slots pre-filled.
On creation of the ring buffer the Disruptor utilises the abstract factory pattern to pre-allocate the entries.
When an entry is claimed, a producer can copy its data into the pre-allocated structure.</p>
</div>
<div class="paragraph">
<p>On most processors there is a very high cost for the remainder calculation on the sequence number, which determines the slot in the ring.
This cost can be greatly reduced by making the ring size a power of 2.
A bit mask of size minus one can be used to perform the remainder operation efficiently.</p>
</div>
<div class="paragraph">
<p>As we described earlier bounded queues suffer from contention at the head and tail of the queue.
The ring buffer data structure is free from this contention and concurrency primitives because these concerns have been teased out into producer and consumer barriers through which the ring buffer must be accessed.
The logic for these barriers is described below.</p>
</div>
<div class="paragraph">
<p>In most common usages of the Disruptor there is usually only one producer.
Typical producers are file readers or network listeners. In cases where there is a single producer there is no contention on sequence/entry allocation.
In more unusual usages where there are multiple producers, producers will race one another to claim the next entry in the ring-buffer.
Contention on claiming the next available entry can be managed with a simple CAS operation on the sequence number for that slot.</p>
</div>
<div class="paragraph">
<p>Once a producer has copied the relevant data to the claimed entry it can make it public to consumers by committing the sequence.
This can be done without CAS by a simple busy spin until the other producers have reached this sequence in their own commit.
Then this producer can advance the cursor signifying the next available entry for consumption.
Producers can avoid wrapping the ring by tracking the sequence of consumers as a simple read operation before they write to the ring buffer.</p>
</div>
<div class="paragraph">
<p>Consumers wait for a sequence to become available in the ring buffer before they read the entry.
Various strategies can be employed while waiting.
If CPU resource is precious they can wait on a condition variable within a lock that gets signalled by the producers.
This obviously is a point of contention and only to be used when CPU resource is more important than latency or throughput.
The consumers can also loop checking the cursor which represents the currently available sequence in the ring buffer.
This could be done with or without a thread yield by trading CPU resource against latency.
This scales very well as we have broken the contended dependency between the producers and consumers if we do not use a lock and condition variable.
Lock free multi-producer – multi-consumer queues do exist but they require multiple CAS operations on the head, tail, size counters.
The Disruptor does not suffer this CAS contention.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sequencing">3.3. Sequencing</h3>
<div class="paragraph">
<p>Sequencing is the core concept to how the concurrency is managed in the Disruptor.
Each producer and consumer works off a strict sequencing concept for how it interacts with the ring buffer.
Producers claim the next slot in sequence when claiming an entry in the ring.
This sequence of the next available slot can be a simple counter in the case of only one producer or an atomic counter updated using CAS operations in the case of multiple producers.
Once a sequence value is claimed, this entry in the ring buffer is now available to be written to by the claiming producer.
When the producer has finished updating the entry it can commit the changes by updating a separate counter which represents the cursor on the ring buffer for the latest entry available to consumers.
The ring buffer cursor can be read and written in a busy spin by the producers using memory barrier without requiring a CAS operation as below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="kt">long</span> <span class="n">expectedSequence</span> <span class="o">=</span> <span class="n">claimedSequence</span> <span class="err">–</span> <span class="mi">1</span><span class="o">;</span>
<span class="k">while</span> <span class="o">(</span><span class="n">cursor</span> <span class="o">!=</span> <span class="n">expectedSequence</span><span class="o">)</span>
<span class="o">{</span>
  <span class="c1">// busy spin</span>
<span class="o">}</span>

<span class="n">cursor</span> <span class="o">=</span> <span class="n">claimedSequence</span><span class="o">;</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Consumers wait for a given sequence to become available by using a memory barrier to read the cursor.
Once the cursor has been updated the memory barriers ensure the changes to the entries in the ring buffer are visible to the consumers who have waited on the cursor advancing.</p>
</div>
<div class="paragraph">
<p>Consumers each contain their own sequence which they update as they process entries from the ring buffer.
These consumer sequences allow the producers to track consumers to prevent the ring from wrapping.
Consumer sequences also allow consumers to coordinate work on the same entry in an ordered manner</p>
</div>
<div class="paragraph">
<p>In the case of having only one producer, and regardless of the complexity of the consumer graph, no locks or CAS operations are required.
The whole concurrency coordination can be achieved with just memory barriers on the discussed sequences.</p>
</div>
</div>
<div class="sect2">
<h3 id="_batching_effect">3.4. Batching Effect</h3>
<div class="paragraph">
<p>When consumers are waiting on an advancing cursor sequence in the ring buffer an interesting opportunity arises that is not possible with queues.
If the consumer finds the ring buffer cursor has advanced a number of steps since it last checked it can process up to that sequence without getting involved in the concurrency mechanisms.
This results in the lagging consumer quickly regaining pace with the producers when the producers burst ahead thus balancing the system.
This type of batching increases throughput while reducing and smoothing latency at the same time.
Based on our observations, this effect results in a close to constant time for latency regardless of load, up until the memory sub-system is saturated, and then the profile is linear following Little’s Law <sup class="footnote" id="_footnote_littleslaw">[<a id="_footnoteref_6" class="footnote" href="#_footnotedef_6" title="View footnote.">6</a>]</sup>.
This is very different to the &#8220;J&#8221; curve effect on latency we have observed with queues as load increases.</p>
</div>
</div>
<div class="sect2">
<h3 id="_dependency_graphs">3.5. Dependency Graphs</h3>
<div class="paragraph">
<p>A queue represents the simple one step pipeline dependency between producers and consumers.
If the consumers form a chain or graph-like structure of dependencies then queues are required between each stage of the graph.
This incurs the fixed costs of queues many times within the graph of dependent stages.
When designing the LMAX financial exchange our profiling showed that taking a queue based approach resulted in queuing costs dominating the total execution costs for processing a transaction.</p>
</div>
<div class="paragraph">
<p>Because the producer and consumer concerns are separated with the Disruptor pattern, it is possible to represent a complex graph of dependencies between consumers while only using a single ring buffer at the core.
This results in greatly reduced fixed costs of execution thus increasing throughput while reducing latency.</p>
</div>
<div class="paragraph">
<p>A single ring buffer can be used to store entries with a complex structure representing the whole workflow in a cohesive place.
Care must be taken in the design of such a structure so that the state written by independent consumers does not result in false sharing of cache lines.</p>
</div>
</div>
<div class="sect2">
<h3 id="_disruptor_class_diagram">3.6. Disruptor Class Diagram</h3>
<div class="paragraph">
<p>The core relationships in the Disruptor framework are depicted in the class diagram below.
This diagram leaves out the convenience classes which can be used to simplify the programming model.
After the dependency graph is constructed the programming model is simple.
Producers claim entries in sequence via a <code>ProducerBarrier</code>, write their changes into the claimed entry, then commit that entry back via the <code>ProducerBarrier</code> making them available for consumption.
As a consumer all one needs do is provide a <code>BatchHandler</code> implementation that receives call backs when a new entry is available.
This resulting programming model is event based having a lot of similarities to the Actor Model.</p>
</div>
<div class="paragraph">
<p>Separating the concerns normally conflated in queue implementations allows for a more flexible design.
A <code>RingBuffer</code> exists at the core of the Disruptor pattern providing storage for data exchange without contention.
The concurrency concerns are separated out for the producers and consumers interacting with the <code>RingBuffer</code>.
The <code>ProducerBarrier</code> manages any concurrency concerns associated with claiming slots in the ring buffer, while tracking dependant consumers to prevent the ring from wrapping.
The <code>ConsumerBarrier</code> notifies consumers when new entries are available, and Consumers can be constructed into a graph of dependencies representing multiple stages in a processing pipeline.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/classdiagram.png" alt="classdiagram">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_code_example">3.7. Code Example</h3>
<div class="paragraph">
<p>The code below is an example of a single producer and single consumer using the convenience interface <code>BatchHandler</code> for implementing a consumer.
The consumer runs on a separate thread receiving entries as they become available.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="c1">// Callback handler which can be implemented by consumers</span>
<span class="kd">final</span> <span class="nc">BatchHandler</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;</span> <span class="n">batchHandler</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">BatchHandler</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;()</span>
<span class="o">{</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">onAvailable</span><span class="o">(</span><span class="kd">final</span> <span class="nc">ValueEntry</span> <span class="n">entry</span><span class="o">)</span> <span class="kd">throws</span> <span class="nc">Exception</span>
<span class="o">{</span>
<span class="c1">// process a new entry as it becomes available.</span>
<span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onEndOfBatch</span><span class="o">()</span> <span class="kd">throws</span> <span class="nc">Exception</span>
    <span class="o">{</span>
        <span class="c1">// useful for flushing results to an IO device if necessary.</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onCompletion</span><span class="o">()</span>
    <span class="o">{</span>
        <span class="c1">// do any necessary clean up before shutdown</span>
    <span class="o">}</span>
<span class="o">};</span>

<span class="nc">RingBuffer</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;</span> <span class="n">ringBuffer</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">RingBuffer</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;(</span><span class="nc">ValueEntry</span><span class="o">.</span><span class="na">ENTRY_FACTORY</span><span class="o">,</span> <span class="no">SIZE</span><span class="o">,</span>
                               <span class="nc">ClaimStrategy</span><span class="o">.</span><span class="na">Option</span><span class="o">.</span><span class="na">SINGLE_THREADED</span><span class="o">,</span>
                               <span class="nc">WaitStrategy</span><span class="o">.</span><span class="na">Option</span><span class="o">.</span><span class="na">YIELDING</span><span class="o">);</span>
<span class="nc">ConsumerBarrier</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;</span> <span class="n">consumerBarrier</span> <span class="o">=</span> <span class="n">ringBuffer</span><span class="o">.</span><span class="na">createConsumerBarrier</span><span class="o">();</span>
<span class="nc">BatchConsumer</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;</span> <span class="n">batchConsumer</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">BatchConsumer</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;(</span><span class="n">consumerBarrier</span><span class="o">,</span> <span class="n">batchHandler</span><span class="o">);</span>
<span class="nc">ProducerBarrier</span><span class="o">&lt;</span><span class="nc">ValueEntry</span><span class="o">&gt;</span> <span class="n">producerBarrier</span> <span class="o">=</span> <span class="n">ringBuffer</span><span class="o">.</span><span class="na">createProducerBarrier</span><span class="o">(</span><span class="n">batchConsumer</span><span class="o">);</span>

<span class="c1">// Each consumer can run on a separate thread</span>
<span class="no">EXECUTOR</span><span class="o">.</span><span class="na">submit</span><span class="o">(</span><span class="n">batchConsumer</span><span class="o">);</span>

<span class="c1">// Producers claim entries in sequence</span>
<span class="nc">ValueEntry</span> <span class="n">entry</span> <span class="o">=</span> <span class="n">producerBarrier</span><span class="o">.</span><span class="na">nextEntry</span><span class="o">();</span>

<span class="c1">// copy data into the entry container</span>

<span class="c1">// make the entry available to consumers</span>
<span class="n">producerBarrier</span><span class="o">.</span><span class="na">commit</span><span class="o">(</span><span class="n">entry</span><span class="o">);</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_throughput_performance_testing">4. Throughput Performance Testing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As a reference we choose Doug Lea’s excellent <code>java.util.concurrent.ArrayBlockingQueue</code><sup class="footnote" id="_footnote_arrayblockingqueue">[<a id="_footnoteref_7" class="footnote" href="#_footnotedef_7" title="View footnote.">7</a>]</sup> which has the highest performance of any bounded queue based on our testing.
The tests are conducted in a blocking programming style to match that of the Disruptor.
The tests cases detailed below are available in the Disruptor open source project.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
running the tests requires a system capable of executing at least 4 threads in parallel.
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/unicast1p1c.png" alt="unicast1p1c">
</div>
<div class="title">Figure 1. Unicast: 1P – 1C</div>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/threestep1p3c.png" alt="threestep1p3c">
</div>
<div class="title">Figure 2. Three Step Pipeline: 1P – 3C</div>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/sequencer3p1c.png" alt="sequencer3p1c">
</div>
<div class="title">Figure 3. Sequencer: 3P – 1C</div>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/multicast1p3c.png" alt="multicast1p3c">
</div>
<div class="title">Figure 4. Multicast: 1P – 3C</div>
</div>
<div class="imageblock">
<div class="content">
<img src="./resources/images/diamond1p3c.png" alt="diamond1p3c">
</div>
<div class="title">Figure 5. Diamond: 1P – 3C</div>
</div>
<div class="paragraph">
<p>For the above configurations an <code>ArrayBlockingQueue</code> was applied for each arc of data flow compared to barrier configuration with the Disruptor.
The following table shows the performance results in operations per second using a Java 1.6.0_25 64-bit Sun JVM, Windows 7, Intel Core i7 860 @ 2.8 GHz without HT and Intel Core i7-2720QM, Ubuntu 11.04, and taking the best of 3 runs when processing 500 million messages.
Results can vary substantially across different JVM executions and the figures below are not the highest we have observed.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Comparative throughput (in ops per sec)</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top" colspan="2">Nehalem 2.8Ghz – Windows 7 SP1 64-bit</th>
<th class="tableblock halign-left valign-top" colspan="2">Sandy Bridge 2.2Ghz – Linux 2.6.38 64-bit</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"></td>
<th class="tableblock halign-left valign-top"><p class="tableblock">ABQ</p></th>
<th class="tableblock halign-left valign-top"><p class="tableblock">Disruptor</p></th>
<th class="tableblock halign-left valign-top"><p class="tableblock">ABQ</p></th>
<th class="tableblock halign-left valign-top"><p class="tableblock">Disruptor</p></th>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unicast: 1P – 1C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,339,256</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">25,998,336</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4,057,453</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">22,381,378</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pipeline: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,128,918</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">16,806,157</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,006,903</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">15,857,913</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sequencer: 3P – 1C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,539,531</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13,403,268</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,056,118</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">14,540,519</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multicast: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,077,384</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">9,377,871</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">260,733</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10,860,121</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Diamond: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,113,941</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">16,143,613</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,082,725</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">15,295,197</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Comparative throughput updated for modern hardware (in ops per sec)</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top" colspan="3">AMD EPYC 9374F – Linux 5.4.277 – OpenJDK 11.0.24</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"></td>
<th class="tableblock halign-left valign-top"><p class="tableblock">ABQ</p></th>
<th class="tableblock halign-left valign-top"><p class="tableblock">Disruptor 3</p></th>
<th class="tableblock halign-left valign-top"><p class="tableblock">Disruptor 4</p></th>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unicast: 1P – 1C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">20,895,148</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">134,553,283</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">160,359,204</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pipeline: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,216,647</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">76,068,766</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">101,317,122</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sequencer: 3P – 1C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">18,791,340</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">16,010,759</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">29,726,516</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multicast: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,355,379</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">68,157,033</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">70,018,204</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Diamond: 1P – 3C</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3,433,665</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">61,229,488</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">63,123,343</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_latency_performance_testing">5. Latency Performance Testing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To measure latency we take the three stage pipeline and generate events at less than saturation.
This is achieved by waiting 1 microsecond after injecting an event before injecting the next and repeating 50 million times.
To time at this level of precision it is necessary to use time stamp counters from the CPU.
We chose CPUs with an invariant TSC because older processors suffer from changing frequency due to power saving and sleep states.
Intel Nehalem and later processors use an invariant TSC which can be accessed by the latest Oracle JVMs running on Ubuntu 11.04.
No CPU binding has been employed for this test.
For comparison we use the ArrayBlockingQueue once again.
We could have used ConcurrentLinkedQueueviii which is likely to give better results but we want to use a bounded queue implementation to ensure producers do not outpace consumers by creating back pressure.
The results below are for 2.2Ghz Core i7-2720QM running Java 1.6.0_25 64-bit on Ubuntu 11.04.
Mean latency per hop for the Disruptor comes out at 52 nanoseconds compared to 32,757 nanoseconds for ArrayBlockingQueue.
Profiling shows the use of locks and signalling via a condition variable are the main cause of latency for the ArrayBlockingQueue.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 4. Comparative Latency in three stage pipeline</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">Array Blocking Queue (ns)</th>
<th class="tableblock halign-left valign-top">Disruptor (ns)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Min Latency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">145</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">29</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mean Latency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">32,757</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">52</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">99% observations less than</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,097,152</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">128</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">99.99% observations less than</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4,194,304</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">8,192</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Max Latency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,069,086</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">175,567</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">6. Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Disruptor is a major step forward for increasing throughput, reducing latency between concurrent execution contexts and ensuring predictable latency, an important consideration in many applications.
Our testing shows that it out-performs comparable approaches for exchanging data between threads.
We believe that this is the highest performance mechanism for such data exchange.
By concentrating on a clean separation of the concerns involved in cross-thread data exchange, by eliminating write contention, minimizing read contention and ensuring that the code worked well with the caching employed by modern processors, we have created a highly efficient mechanism for exchanging data between threads in any application.</p>
</div>
<div class="paragraph">
<p>The batching effect that allows consumers to process entries up to a given threshold, without any contention, introduces a new characteristic in high performance systems.
For most systems, as load and contention increase there is an exponential increase in latency, the characteristic &#8220;J&#8221; curve.
As load increases on the Disruptor, latency remains almost flat until saturation occurs of the memory sub-system.</p>
</div>
<div class="paragraph">
<p>We believe that the Disruptor establishes a new benchmark for high-performance computing and is very well placed to continue to take advantage of current trends in processor and computer design.</p>
</div>
<div class="paragraph">
<p>View the original PDF of this paper <a href="./files/Disruptor-1.0.pdf">here</a>.</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. Staged Event-Driven Architecture – <a href="https://en.wikipedia.org/wiki/Staged_event-driven_architecture" class="bare">https://en.wikipedia.org/wiki/Staged_event-driven_architecture</a>
</div>
<div class="footnote" id="_footnotedef_2">
<a href="#_footnoteref_2">2</a>. Actor model – <a href="http://dspace.mit.edu/handle/1721.1/6952" class="bare">http://dspace.mit.edu/handle/1721.1/6952</a>
</div>
<div class="footnote" id="_footnotedef_3">
<a href="#_footnoteref_3">3</a>. Java Memory Model - <a href="https://jcp.org/en/jsr/detail?id=133" class="bare">https://jcp.org/en/jsr/detail?id=133</a>
</div>
<div class="footnote" id="_footnotedef_4">
<a href="#_footnoteref_4">4</a>. Phasers - <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Phaser.html" class="bare">https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Phaser.html</a>
</div>
<div class="footnote" id="_footnotedef_5">
<a href="#_footnoteref_5">5</a>. Value Types - <a href="https://blogs.oracle.com/jrose/tuples-in-the-vm" class="bare">https://blogs.oracle.com/jrose/tuples-in-the-vm</a>
</div>
<div class="footnote" id="_footnotedef_6">
<a href="#_footnoteref_6">6</a>. Little’s Law - <a href="https://en.wikipedia.org/wiki/Little%27s_law" class="bare">https://en.wikipedia.org/wiki/Little%27s_law</a>
</div>
<div class="footnote" id="_footnotedef_7">
<a href="#_footnoteref_7">7</a>. ArrayBlockingQueue - <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ArrayBlockingQueue.html" class="bare">https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ArrayBlockingQueue.html</a>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 1.0<br>
Last updated 2024-09-13 18:40:29 UTC
</div>
</div>
</body>
</html>